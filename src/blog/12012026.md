---
title: "Linear regression and optimization in deep learning"
pubDate: 2026-01-12
author: Otto
tags: ["exchange studies", "deep learning"]
---

Today my teacher in the **deep learning** course asked us to write shortly about **linear regression** and **optimization**, especially if these topics are not yet familiar. I have already done some deep learning studies at my home university, but I thought it would be a good idea to refresh my understanding of the course topic and these two specific concepts.

Let’s start with **deep learning**. By my current understanding, deep learning is a subset of machine learning. It uses neural networks, which are algorithm structures that can solve complex problems without programming every rule explicitly. For example, consider an autonomous car and traffic lights. The task is to make the vehicle recognize different traffic light situations in a city. To keep it simple, there are three conditions: green light, red light and no light. By collecting a dataset of images of these conditions and training a deep learning model, we can teach the car to recognize traffic lights instead of manually programming every single intersection. This is the core idea of deep learning: learning from data rather than hard coded rules.

**Linear regression** is a technique used to find a relationship between given variables. It works well when the relationship in the dataset is roughly linear, but it is also a fundamental part of deep learning neural networks. Let’s take a simple example of predicting house prices. Price is the output variable, and we choose size as the input variable. If we plot house sizes on the x-axis and prices on the y-axis, each house becomes a dot on the graph. Linear regression then finds a straight line that fits these dots so that the prediction error is as small as possible. In neural networks, a similar idea is used inside each neuron: a neuron takes multiple input variables, computes a weighted sum, and passes the result forward as an output, which can become the input for the next neuron. So even though linear regression alone is not enough to solve the traffic light recognition task, it works as a fundamental building block inside deep learning models.

**Optimization** is an essential part of training machine learning model. In this simple house pricing example we use only one input variable, size. If we plot the dataset as dots on a graph, it is possible to draw fairly good straight line by eyeballing. However, optimization allows the computer to find the mathematically best-fitting line by minimizing the prediction error over all data points. When we add more input variables such as location, age, and number of rooms, the straight line becomes a multi-dimensional surface called a hyperplane, which we can no longer visualize. Even then, optimization works in the same way. It adjusts the model’s parameters step by step to find the best possible fit to the data.
